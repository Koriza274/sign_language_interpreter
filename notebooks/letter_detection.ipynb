{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ff7cbf-c0b0-4d05-bcf3-71c7ddd83021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 17:53:33.446634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 17:53:33.518612: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 17:53:33.541812: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 17:53:33.699871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 17:53:34.981722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d2742-a79a-4446-9bd1-71084cc4ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(image, brightness=40, contrast=1.0):\n",
    "    # Convert to float to prevent clipping\n",
    "    img = image.astype(np.float32)\n",
    "    # Adjust brightness and contrast\n",
    "    img = img * contrast + brightness\n",
    "    # Clip to keep pixel values between 0 and 255 and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44201113-4264-45d5-8586-d447b2b5ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the brightness and contrast adjustment function\n",
    "def adjust_brightness_contrast(image, brightness=40, contrast=1.0):\n",
    "    # Convert to float to prevent clipping\n",
    "    img = image.astype(np.float32)\n",
    "    # Adjust brightness and contrast\n",
    "    img = img * contrast + brightness\n",
    "    # Clip to keep pixel values between 0 and 255 and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# Ranges for brightness and contrast to test\n",
    "brightness_values = range(-20, 51, 5)  # e.g., from -50 to 50 in steps of 20\n",
    "contrast_values = [0.5, 0.75, 1.0, 1.25, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85080eab-f7df-47db-8619-0454aa6dbed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730480022.219247   52220 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1730480022.237120   52334 gl_context.cc:357] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: D3D12 (NVIDIA Quadro RTX 3000)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1730480022.266529   52315 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1730480022.281514   52317 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path where images are stored for each letter in the ASL alphabet\n",
    "#data_dir = \"C:/Users/rober/Downloads/asl_alphabet_dataset/asl_alphabet_train\" # Change according to local dataset\n",
    "data_dir = \"../raw_data/asl_alphabet_dataset/asl_alphabet_train\"\n",
    "\n",
    "landmark_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77e6f2-cef5-43bc-b140-f39af7deed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect landmarks for each letter\n",
    "for letter in os.listdir(data_dir):\n",
    "    #if letter==\"C\":\n",
    "    #    break\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        #if i >= 300:\n",
    "        #    break\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        landmarks_found = False\n",
    "        for brightness in brightness_values:\n",
    "            for contrast in contrast_values:\n",
    "                # Adjust brightness and contrast\n",
    "                adjusted_image = adjust_brightness_contrast(image, brightness, contrast)                \n",
    "                # Run MediaPipe hand detection\n",
    "                results = hands.process(cv2.cvtColor(adjusted_image, cv2.COLOR_BGR2RGB))\n",
    "                # Check for hand landmarks and store them\n",
    "                if results.multi_hand_landmarks:\n",
    "                    landmarks = []\n",
    "                    landmarks_found = True\n",
    "                    for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                        landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "                    landmark_data.append(landmarks)\n",
    "                    labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "        #img = adjust_brightness_contrast(img, 20, 0.7)\n",
    "        #img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #results = hands.process(img_rgb)      \n",
    "        \n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbe77d0-18a4-4ae7-93b6-cfca0d3529db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1730480032.878550   52319 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/home/robert/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "X\n",
      "M\n",
      "F\n",
      "E\n",
      "I\n",
      "R\n",
      "U\n",
      "G\n",
      "P\n",
      "N\n",
      "Z\n",
      "S\n",
      "K\n",
      "D\n",
      "Q\n",
      "J\n",
      "del\n",
      "nothing\n",
      "V\n",
      "T\n",
      "C\n",
      "Y\n",
      "B\n",
      "space\n",
      "W\n",
      "O\n",
      "L\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "# Collect landmarks for each letter\n",
    "for letter in os.listdir(data_dir):\n",
    "    #if letter==\"C\":\n",
    "    #    break\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        if i >= 300:\n",
    "            break\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        #img = adjust_brightness_contrast(img, 40, 1)\n",
    "        #img = adjust_brightness_contrast(img, 20, 0.7)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        \n",
    "        # Check for hand landmarks and store them\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "            landmark_data.append(landmarks)\n",
    "            labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1007c947-64fc-4540-a3b0-5590a164f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H' 'H' 'H' ... 'A' 'A' 'A']\n"
     ]
    }
   ],
   "source": [
    "# Convert to arrays for model input\n",
    "landmark_data = np.array(landmark_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(labels)\n",
    "# Save the arrays to .npy files\n",
    "np.save(\"landmark_data.npy\", landmark_data)\n",
    "np.save(\"labels.npy\", labels)\n",
    "\n",
    "# Normalize landmarks between 0 and 1\n",
    "landmark_data = landmark_data / np.max(landmark_data)\n",
    "\n",
    "# Encode labels as integers and convert to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e139befe-5de2-4754-8169-3525b8dc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(landmark_data, labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17690fa-0e8f-4cd8-92d8-cce701238b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/.pyenv/versions/3.10.6/envs/sign_language_interpreter/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1730479678.048208   47580 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3719 MB memory:  -> device: 0, name: Quadro RTX 3000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Build 1D CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5706e7e1-10e3-4d16-b2d7-7ea0234e116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730479685.179855   49050 service.cc:148] XLA service 0x7fdd240066c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730479685.179892   49050 service.cc:156]   StreamExecutor device (0): Quadro RTX 3000, Compute Capability 7.5\n",
      "2024-11-01 17:48:05.211134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1730479685.345128   49050 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-01 17:48:06.612741: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 78/151\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0371 - loss: 3.3335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730479687.271523   49050 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0486 - loss: 3.3095 - val_accuracy: 0.3055 - val_loss: 2.8561\n",
      "Epoch 2/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2695 - loss: 2.4876 - val_accuracy: 0.7649 - val_loss: 1.0165\n",
      "Epoch 3/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5565 - loss: 1.3107 - val_accuracy: 0.8800 - val_loss: 0.6356\n",
      "Epoch 4/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6871 - loss: 0.9279 - val_accuracy: 0.8932 - val_loss: 0.5116\n",
      "Epoch 5/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7566 - loss: 0.7446 - val_accuracy: 0.9222 - val_loss: 0.3686\n",
      "Epoch 6/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7950 - loss: 0.6274 - val_accuracy: 0.9238 - val_loss: 0.3455\n",
      "Epoch 7/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.5717 - val_accuracy: 0.9437 - val_loss: 0.3003\n",
      "Epoch 8/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8340 - loss: 0.5235 - val_accuracy: 0.9553 - val_loss: 0.2540\n",
      "Epoch 9/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8573 - loss: 0.4579 - val_accuracy: 0.9545 - val_loss: 0.2291\n",
      "Epoch 10/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.4347 - val_accuracy: 0.9462 - val_loss: 0.2398\n",
      "Epoch 11/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8684 - loss: 0.4407 - val_accuracy: 0.9694 - val_loss: 0.2032\n",
      "Epoch 12/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8667 - loss: 0.4110 - val_accuracy: 0.9702 - val_loss: 0.1753\n",
      "Epoch 13/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8932 - loss: 0.3596 - val_accuracy: 0.9735 - val_loss: 0.1721\n",
      "Epoch 14/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9007 - loss: 0.3272 - val_accuracy: 0.9677 - val_loss: 0.1688\n",
      "Epoch 15/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8950 - loss: 0.3300 - val_accuracy: 0.9702 - val_loss: 0.1751\n",
      "Epoch 16/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9115 - loss: 0.3137 - val_accuracy: 0.9710 - val_loss: 0.1650\n",
      "Epoch 17/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.3441 - val_accuracy: 0.9652 - val_loss: 0.1660\n",
      "Epoch 18/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9158 - loss: 0.2720 - val_accuracy: 0.9743 - val_loss: 0.1416\n",
      "Epoch 19/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9050 - loss: 0.3104 - val_accuracy: 0.9652 - val_loss: 0.1482\n",
      "Epoch 20/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.2518 - val_accuracy: 0.9694 - val_loss: 0.1443\n",
      "Epoch 21/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9109 - loss: 0.2660 - val_accuracy: 0.9694 - val_loss: 0.1418\n",
      "Epoch 22/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9150 - loss: 0.2747 - val_accuracy: 0.9702 - val_loss: 0.1292\n",
      "Epoch 23/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9205 - loss: 0.2580 - val_accuracy: 0.9702 - val_loss: 0.1258\n",
      "Epoch 24/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9225 - loss: 0.2385 - val_accuracy: 0.9834 - val_loss: 0.1136\n",
      "Epoch 25/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9120 - loss: 0.2633 - val_accuracy: 0.9768 - val_loss: 0.1223\n",
      "Epoch 26/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9248 - loss: 0.2438 - val_accuracy: 0.9793 - val_loss: 0.1189\n",
      "Epoch 27/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9216 - loss: 0.2537 - val_accuracy: 0.9727 - val_loss: 0.1255\n",
      "Epoch 28/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9250 - loss: 0.2375 - val_accuracy: 0.9801 - val_loss: 0.1211\n",
      "Epoch 29/40\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9283 - loss: 0.2187 - val_accuracy: 0.9727 - val_loss: 0.1212\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience = 5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train[..., np.newaxis], y_train, epochs=40, batch_size=32, validation_data=(X_test[..., np.newaxis], y_test), callbacks=es)\n",
    "#model.save(\"asl_sign_language_model.h5\")\n",
    "save_model(model, 'asl_sign_language_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ec251-7631-4d01-9a80-8277fac3ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "#model = tf.keras.models.load_model(\"asl_sign_language_model.h5\")\n",
    "model = tf.keras.models.load_model(\"asl_sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c7f9-e77c-4da6-8686-5f9e4c923722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as integers and convert to categorical\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6dbce-1f4b-4e88-8855-adfa3a61cd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "sequence = []\n",
    "sequence_length = 1  # Set sequence length to 10 frames for rolling window approach - not working yet\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for lm in results.multi_hand_landmarks[0].landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, \n",
    "            results.multi_hand_landmarks[0], \n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Append new frame landmarks to sequence\n",
    "        sequence.append(landmarks)\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        # Predict only if the sequence is full\n",
    "        if len(sequence) == sequence_length:\n",
    "            sequence_input = np.array(sequence).flatten()[np.newaxis, ..., np.newaxis]\n",
    "            prediction = model.predict(sequence_input)\n",
    "            predicted_label_index = np.argmax(prediction)\n",
    "            predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "            confidence = prediction[0][predicted_label_index] * 100  # Get confidence percentage\n",
    "\n",
    "            # Display prediction and confidence\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_label[0]} ({confidence:.2f}%)\", \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)  # Black text color\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow(\"ASL Sign Language Detection\", frame)\n",
    "    \n",
    "    # Press 'C' to terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"c\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
