{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff7cbf-c0b0-4d05-bcf3-71c7ddd83021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import imghdr\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44201113-4264-45d5-8586-d447b2b5ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the brightness and contrast adjustment function\n",
    "def adjust_brightness_contrast(image, brightness=40, contrast=1.0):\n",
    "    # Convert to float to prevent clipping\n",
    "    img = image.astype(np.float32)\n",
    "    # Adjust brightness and contrast\n",
    "    img = img * contrast + brightness\n",
    "    # Clip to keep pixel values between 0 and 255 and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# Ranges for brightness and contrast to test\n",
    "brightness_values = range(-20, 71, 5)  # e.g., from -50 to 50 in steps of 20\n",
    "contrast_values = [0.5, 0.75, 1.0, 1.25, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19fa8d-9a87-4928-a515-ccb8402a69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(file_path):\n",
    "    # Returns the type of image if valid, otherwise None\n",
    "    return imghdr.what(file_path) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85080eab-f7df-47db-8619-0454aa6dbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path where images are stored for each letter in the ASL alphabet\n",
    "data_dir = \"../raw_data/ASL_Alphabet_Dataset/asl_alphabet_train\"\n",
    "\n",
    "landmark_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e4f4a-f1b8-4aaf-be04-c38ae53c834a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect landmarks for each letter\n",
    "for letter in os.listdir(data_dir):\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    letter_count = 0\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        if not is_image(os.path.join(letter_dir, img_path)):\n",
    "            continue\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            letter_count += 1\n",
    "            landmarks = []\n",
    "            for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "            landmark_data.append(landmarks)\n",
    "            labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "    print(letter, letter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77e6f2-cef5-43bc-b140-f39af7deed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect landmarks for each letter with improved brighness and contrast as an alternative\n",
    "for letter in os.listdir(data_dir):\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    letter_count = 0\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        if not is_image(os.path.join(letter_dir, img_path)):\n",
    "            continue\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        landmarks_found = False\n",
    "        for brightness in brightness_values:\n",
    "            for contrast in contrast_values:\n",
    "                # Adjust brightness and contrast\n",
    "                adjusted_image = adjust_brightness_contrast(img, brightness, contrast)\n",
    "                # Run MediaPipe hand detection\n",
    "                results = hands.process(cv2.cvtColor(adjusted_image, cv2.COLOR_BGR2RGB))\n",
    "                # Check for hand landmarks and store them\n",
    "                if results.multi_hand_landmarks:\n",
    "                    landmarks = []\n",
    "                    landmarks_found = True\n",
    "                    letter_count += 1\n",
    "                    for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                        landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "                    landmark_data.append(landmarks)\n",
    "                    labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "                    break\n",
    "            if landmarks_found:\n",
    "                #print(f'Brightness: {brightness}, Contrast: {contrast}')\n",
    "                break\n",
    "    print(letter, letter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007c947-64fc-4540-a3b0-5590a164f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays for model input\n",
    "landmark_data = np.array(landmark_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Save arrays to .npy files\n",
    "np.save(\"landmark_data_large.npy\", landmark_data)\n",
    "np.save(\"labels_large.npy\", labels)\n",
    "\n",
    "# Encode labels as integers and convert to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "np.save('label_classes.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c34a6-7036-4581-8699-f2864e3f1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"../models/production_model/labels_large.npy\")\n",
    "landmark_data = np.load(\"../models/production_model/landmark_data_large.npy\")\n",
    "\n",
    "# Normalize landmarks relative to the wrist (landmark 0) for each frame\n",
    "normalized_landmark_data = []\n",
    "for frame in landmark_data:\n",
    "    # Extract wrist coordinates\n",
    "    wrist_x, wrist_y, wrist_z = frame[0], frame[1], frame[2]\n",
    "\n",
    "    # Normalize each landmark in the frame relative to the wrist\n",
    "    normalized_frame = []\n",
    "    for i in range(0, len(frame), 3):  # Iterate over (x, y, z) coordinates\n",
    "        normalized_x = frame[i] - wrist_x\n",
    "        normalized_y = frame[i + 1] - wrist_y\n",
    "        normalized_z = frame[i + 2] - wrist_z\n",
    "        normalized_frame.extend([normalized_x, normalized_y, normalized_z])\n",
    "\n",
    "    normalized_landmark_data.append(normalized_frame)\n",
    "\n",
    "# Convert to numpy array\n",
    "normalized_landmark_data = np.array(normalized_landmark_data)\n",
    "\n",
    "# Encode labels as integers and convert to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for letters\n",
    "y = np.array(labels).tolist()  # Convert numpy array to list of strings\n",
    "y = [\"Unknown\" if (not isinstance(label, str) or len(label) == 0 or pd.isna(label)) else str(label) for label in y]\n",
    "label_encoder = tf.keras.preprocessing.text.Tokenizer()\n",
    "label_encoder.fit_on_texts(y)\n",
    "y = label_encoder.texts_to_sequences(y)\n",
    "y = [label[0] for label in y]  # Flatten the list of lists\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(label_encoder.word_index) + 1)\n",
    "num_classes = len(label_encoder.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139befe-5de2-4754-8169-3525b8dc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_landmark_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96584f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the dimensions of X_train and X_test\n",
    "X_train_expanded = np.expand_dims(X_train, axis=-1)\n",
    "X_test_expanded = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190329dc-1f29-4050-b7b2-f68a0bb85c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data and labels in unison\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Optionally shuffle test data too\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17690fa-0e8f-4cd8-92d8-cce701238b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1D CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a926306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, epochs=1000, validation_split=0.3, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"asl_sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ec251-7631-4d01-9a80-8277fac3ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "#model = tf.keras.models.load_model(\"asl_sign_language_model.h5\")\n",
    "model = tf.keras.models.load_model(\"../models/production_model/asl_sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c7f9-e77c-4da6-8686-5f9e4c923722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as integers and convert to categorical\n",
    "labels = np.load(\"labels.npy\")\n",
    "landmark_data = np.load(\"landmark_data.npy\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6dbce-1f4b-4e88-8855-adfa3a61cd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction on webcam feed\n",
    "# Initialize MediaPipe Hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "scaling_factor = np.max(landmark_data)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "sequence = []\n",
    "sequence_length = 1  # Set sequence length to 10 frames for rolling window approach - not working yet\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        #for lm in results.multi_hand_landmarks[0].landmark:\n",
    "        #    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        # Retrieve wrist coordinates (landmark 0)\n",
    "        wrist = results.multi_hand_landmarks[0].landmark[0]\n",
    "\n",
    "        for lm in results.multi_hand_landmarks[0].landmark:\n",
    "            # Normalize each landmark relative to the wrist\n",
    "            normalized_x = lm.x - wrist.x\n",
    "            normalized_y = lm.y - wrist.y\n",
    "            normalized_z = lm.z - wrist.z\n",
    "            landmarks.extend([normalized_x, normalized_y, normalized_z])\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.multi_hand_landmarks[0],\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Append new frame landmarks to sequence\n",
    "        sequence.append(landmarks)\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        # Predict only if the sequence is full\n",
    "        if len(sequence) == sequence_length:\n",
    "            sequence_input = np.array(sequence).flatten()[np.newaxis, ..., np.newaxis]\n",
    "            #sequence_input = sequence_input/scaling_factor\n",
    "            prediction = model.predict(sequence_input)\n",
    "            predicted_label_index = np.argmax(prediction)\n",
    "            predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "            confidence = prediction[0][predicted_label_index] * 100  # Get confidence percentage\n",
    "\n",
    "            # Display prediction and confidence\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_label[0]} ({confidence:.2f}%)\",\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)  # Black text color\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"ASL Sign Language Detection\", frame)\n",
    "\n",
    "    # Press 'C' to terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"c\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce3256-35cb-48b4-87ba-eef60f61b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on images from directory\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"asl_sign_language_model_large.keras\")\n",
    "\n",
    "labels = np.load(\"labels.npy\")\n",
    "#landmark_data = np.load(\"landmark_data.npy\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Initialize MediaPipe Hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Root directory with test images in subdirectories\n",
    "root_test_dir = \"../raw_data/test_set_pics\"\n",
    "\n",
    "# Initialize a list to store processed images\n",
    "processed_images = []\n",
    "\n",
    "# Iterate through each subdirectory in the root directory\n",
    "for subdir_name in os.listdir(root_test_dir):\n",
    "    subdir_path = os.path.join(root_test_dir, subdir_name)\n",
    "\n",
    "    # Ensure it is a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "        actual_label = subdir_name  # Use the subdirectory name as the actual label\n",
    "\n",
    "        # Iterate through each image in the subdirectory\n",
    "        for img_name in os.listdir(subdir_path):\n",
    "            img_path = os.path.join(subdir_path, img_name)\n",
    "            print(img_path)\n",
    "            img = cv2.imread(img_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(img_rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Prepare landmarks for model prediction\n",
    "                landmarks = []\n",
    "                wrist = results.multi_hand_landmarks[0].landmark[0]\n",
    "                for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                    # Normalize landmarks relative to the wrist\n",
    "                    normalized_x = lm.x - wrist.x\n",
    "                    normalized_y = lm.y - wrist.y\n",
    "                    normalized_z = lm.z - wrist.z\n",
    "                    landmarks.extend([normalized_x, normalized_y, normalized_z])\n",
    "\n",
    "                # Prepare the input for the model\n",
    "                sequence_input = np.array(landmarks)[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "                # Make prediction\n",
    "                prediction = model.predict(sequence_input)\n",
    "                predicted_label_index = np.argmax(prediction)\n",
    "                predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "                confidence = prediction[0][predicted_label_index] * 100  # Confidence percentage\n",
    "\n",
    "                # Draw landmarks on the image\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    img,\n",
    "                    results.multi_hand_landmarks[0],\n",
    "                    mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "                # Add actual label, predicted label, and confidence to the image\n",
    "                cv2.putText(img, f\"Actual: {actual_label}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), 6)  # Green text color\n",
    "                cv2.putText(img, f\"Predicted: {predicted_label[0]} ({confidence:.2f}%)\", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 0, 255), 6)  # Red text color\n",
    "\n",
    "\n",
    "            # Append processed image to the list\n",
    "            processed_images.append(img)\n",
    "\n",
    "# Display each processed image and check for \"ESC\" key press\n",
    "for processed_img in processed_images:\n",
    "    cv2.imshow(\"Processed Test Image\", processed_img)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
