{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432697b0-3d21-4a39-8daa-285d18e6d5fc",
   "metadata": {},
   "source": [
    "# Media Pipe + Data Modelling for Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6f8c6625-f57b-43fd-b262-108ced1ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, Flatten, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc490c7-91ac-4567-95ec-ea3d8e9bd4ef",
   "metadata": {},
   "source": [
    "## Read this:\n",
    "\n",
    "### check all paths before running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41156c-796d-4e46-8b3d-6d0d3a00ae2f",
   "metadata": {},
   "source": [
    "## Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6eae9fc-ffce-48f3-9407-fd897c3128c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(image, brightness=40, contrast=1.0):\n",
    "    # Convert to float to prevent clipping\n",
    "    img = image.astype(np.float32)\n",
    "    # Adjust brightness and contrast\n",
    "    img = img * contrast + brightness\n",
    "    # Clip to keep pixel values between 0 and 255 and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6c81a1-de21-4076-97c6-97ba58f8e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_landmarks(landmarks):\n",
    "    # Normalize landmarks relative to the wrist (landmark 0) for each frame\n",
    "    normalized_landmark_data = []\n",
    "    for frame in landmarks:\n",
    "        # Extract wrist coordinates\n",
    "        wrist_x, wrist_y, wrist_z = frame[0], frame[1], frame[2]\n",
    "\n",
    "        # Normalize each landmark in the frame relative to the wrist\n",
    "        normalized_frame = []\n",
    "        for i in range(0, len(frame), 3):  # Iterate over (x, y, z) coordinates\n",
    "            normalized_x = frame[i] - wrist_x\n",
    "            normalized_y = frame[i + 1] - wrist_y\n",
    "            normalized_z = frame[i + 2] - wrist_z\n",
    "            normalized_frame.extend([normalized_x, normalized_y, normalized_z])\n",
    "\n",
    "        normalized_landmark_data.append(normalized_frame)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    normalized_landmark_data = np.array(normalized_landmark_data)\n",
    "    \n",
    "    return normalized_landmark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb6a5ac-a4c0-455c-9f45-35e96fb1196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(directory):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,min_detection_confidence=0.4)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    img = cv2.imread(directory)\n",
    "    img = adjust_brightness_contrast(img, 40, 1)\n",
    "    img_rbg =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img_rbg)\n",
    "\n",
    "    sequence = []\n",
    "    sequence_length = 1\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for lm in result.multi_hand_landmarks[0].landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            img,\n",
    "            result.multi_hand_landmarks[0],\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Append new frame landmarks to sequence\n",
    "\n",
    "        sequence.append(landmarks)\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            sequence_input = np.array(sequence)\n",
    "            sequence_input = normalization_landmarks(sequence_input)\n",
    "            sequence_input = sequence_input.flatten()[np.newaxis, ..., np.newaxis]\n",
    "            prediction = model.predict(sequence_input)\n",
    "            predicted_label_index = np.argmax(prediction)\n",
    "            predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "            confidence = prediction[0][predicted_label_index]\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e352bd-2120-4910-9881-522af182b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data_dir):\n",
    "        mp_hands = mp.solutions.hands\n",
    "        hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.4)\n",
    "\n",
    "\n",
    "        labels1 = []\n",
    "        landmark_data1 = []\n",
    "\n",
    "        for letter in os.listdir(test_data_dir):\n",
    "\n",
    "            letter_dir = os.path.join(test_data_dir, letter)\n",
    "            for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "\n",
    "                img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "                img = adjust_brightness_contrast(img, 40, 1)\n",
    "\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(img_rgb)\n",
    "\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    landmarks = []\n",
    "                    for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                        landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                    landmark_data1.append(landmarks)\n",
    "                    labels1.append(letter)\n",
    "        landmark_data1 = np.array(landmark_data1)\n",
    "        labels1 = np.array(labels1)\n",
    "\n",
    "\n",
    "        # Normalize landmarks between 0 and 1\n",
    "        landmark_data1 = normalization_landmarks(landmark_data1)\n",
    "\n",
    "        # Encode labels as integers and convert to categorical\n",
    "\n",
    "        labels_encoded1 = label_encoder.transform(labels1)\n",
    "        labels_categorical1 = to_categorical(labels_encoded1)\n",
    "        landmark_data1 = np.reshape(landmark_data1,(-1,63,1))\n",
    "\n",
    "        return model.evaluate(landmark_data1,labels_categorical1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0825f1-dfcc-4ad4-8151-445cac482851",
   "metadata": {},
   "source": [
    "## MediaPipe Landmark Creation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a16ee-6c1a-4b00-a145-4ce91643b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path where images are stored for each letter in the ASL alphabet\n",
    "data_dir = \"../raw_data/asl_alphabet_train/asl_alphabet_train\" # Change according to local dataset\n",
    "\n",
    "landmark_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca97671-58ea-41b5-aedb-346cc28d2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect landmarks for each letter\n",
    "# This function does not normalize inside. We need to run Normalization function outside\n",
    "for letter in os.listdir(data_dir):\n",
    "    #if letter==\"C\":\n",
    "    #    break\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        #if i >= 300:\n",
    "        #    break\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        img = adjust_brightness_contrast(img, 40, 1)\n",
    "        #img = adjust_brightness_contrast(img, 20, 0.7)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        \n",
    "        # Check for hand landmarks and store them\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "            landmark_data.append(landmarks)\n",
    "            labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc9c0e89-ec69-432e-8c4d-bf09a08ca255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the arrays to .npy files\n",
    "# landmark_data is NOT normalized until now. Only what MediaPipe does.\n",
    "np.save(\"landmark_data_v_large.npy\", landmark_data)\n",
    "np.save(\"labels_v_large.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bababf-26d5-4a0a-920b-0d0095202974",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9123fc-e830-4501-bfb0-4216e8b10629",
   "metadata": {},
   "source": [
    "### If you already have landmark_data and labels you can start running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9249a6ae-5e98-4906-a404-ba1b027e048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.load(\"labels.npy\")\n",
    "#landmark_data = np.load(\"landmark_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080a4daf-62a7-4e0d-abd8-6338b1438984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding as integers and convert to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b51a16-509b-4b5b-847b-58a2f051fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Normalization function:\n",
    "normalized_landmarks = normalization_landmarks(landmark_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad916b4-7b0b-4f02-9d7a-7d85c5885da7",
   "metadata": {},
   "source": [
    "### Model Prep\n",
    "\n",
    "This is only if required but as we are using a different test set I will not test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b4d77cd7-d09c-4a58-b0ba-0bf88833bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_landmarks, labels_categorical, stratify=labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26976d-7b48-4566-b5a6-6c8aad59956f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b53bc5-390d-45c3-9cc0-77a0ceeae3f8",
   "metadata": {},
   "source": [
    "### Building & Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "91427169-b3db-4b6b-84f4-8b08c670a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Model\n",
    "model = Sequential([\n",
    "    # First convolutional block\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fully connected layers\n",
    "    Flatten(),\n",
    "    \n",
    "    # Additional dense layer before the output\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    \n",
    "    # Output layer with softmax activation for classification\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e80e12e0-47ca-49db-910d-e813377dfcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Testing Model\n",
    "model = Sequential([\n",
    "    # First convolutional block\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second convolutional block\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Third convolutional block\n",
    "    Conv1D(256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Fully connected layers\n",
    "    Flatten(),\n",
    "    \n",
    "    # Additional dense layer before the output\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    \n",
    "    # Output layer with softmax activation for classification\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5a92ec85-7f71-410e-b782-8f2f45134afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Model\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "93ac0126-63b4-47f0-ae77-8873625ff756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc6aee-edbe-4392-8699-97c40f2d3517",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fe5ca3bf-efdd-4cfd-aaa4-067e6ff0468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience = 2, \n",
    "                   restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "10f7147c-db2e-43c0-be6b-b44f38b537a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 13ms/step - accuracy: 0.7231 - loss: 0.8944 - val_accuracy: 0.9771 - val_loss: 0.0695\n",
      "Epoch 2/5\n",
      "\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 13ms/step - accuracy: 0.9484 - loss: 0.1643 - val_accuracy: 0.9807 - val_loss: 0.0592\n",
      "Epoch 3/5\n",
      "\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12ms/step - accuracy: 0.9593 - loss: 0.1357 - val_accuracy: 0.9823 - val_loss: 0.0490\n",
      "Epoch 4/5\n",
      "\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 12ms/step - accuracy: 0.9644 - loss: 0.1177 - val_accuracy: 0.9875 - val_loss: 0.0401\n",
      "Epoch 5/5\n",
      "\u001b[1m3594/3594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 13ms/step - accuracy: 0.9686 - loss: 0.1071 - val_accuracy: 0.9892 - val_loss: 0.0371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3ba1e72b0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x, \n",
    "          y, \n",
    "          epochs=5, \n",
    "          batch_size=64, \n",
    "          validation_data=(X_test[..., np.newaxis], y_test), \n",
    "          callbacks=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879041c1-1b05-4219-a24f-bfbf8c2c8c3e",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c0334f17-4a82-4bf3-8b63-cf03c356f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'asl_new_model?.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff40212-50b5-4bff-8140-a57b0e1bda3a",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f8987-f8e4-4d0b-97e0-adc55f89868d",
   "metadata": {},
   "source": [
    "### If you already have a model. You can load it up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c448db-4b57-45d8-85f4-2ea69c93b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# model = tf.keras.models.load_model(\"old_asl_sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7cc1e800-39eb-4ca9-a887-d7d971c8f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730927879.223193 6326681 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1730927879.231553 6457570 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1730927879.239616 6457570 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6003 - loss: 2.4117 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9537979364395142, 0.6811594367027283]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.5-0.6\n",
    "evaluate_model(\"raw_data/test_set_pics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "78795f92-e55f-4315-8290-e3393c3acf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730930179.071587 6326681 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1730930179.078971 6484375 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1730930179.083731 6484374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5698 - loss: 1.8008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5537683963775635, 0.6521739363670349]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"raw_data/test_set_pics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57671340-2b16-4eae-b875-351f0eec1224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
