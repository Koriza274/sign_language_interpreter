{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432697b0-3d21-4a39-8daa-285d18e6d5fc",
   "metadata": {},
   "source": [
    "# Media Pipe + Data Modelling for Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8c6625-f57b-43fd-b262-108ced1ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc490c7-91ac-4567-95ec-ea3d8e9bd4ef",
   "metadata": {},
   "source": [
    "## Read this:\n",
    "\n",
    "### check all paths before running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41156c-796d-4e46-8b3d-6d0d3a00ae2f",
   "metadata": {},
   "source": [
    "## Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eae9fc-ffce-48f3-9407-fd897c3128c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(image, brightness=40, contrast=1.0):\n",
    "    # Convert to float to prevent clipping\n",
    "    img = image.astype(np.float32)\n",
    "    # Adjust brightness and contrast\n",
    "    img = img * contrast + brightness\n",
    "    # Clip to keep pixel values between 0 and 255 and convert back to uint8\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6c81a1-de21-4076-97c6-97ba58f8e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_landmarks(landmarks):\n",
    "    # Normalize landmarks relative to the wrist (landmark 0) for each frame\n",
    "    normalized_landmark_data = []\n",
    "    for frame in landmarks:\n",
    "        # Extract wrist coordinates\n",
    "        wrist_x, wrist_y, wrist_z = frame[0], frame[1], frame[2]\n",
    "\n",
    "        # Normalize each landmark in the frame relative to the wrist\n",
    "        normalized_frame = []\n",
    "        for i in range(0, len(frame), 3):  # Iterate over (x, y, z) coordinates\n",
    "            normalized_x = frame[i] - wrist_x\n",
    "            normalized_y = frame[i + 1] - wrist_y\n",
    "            normalized_z = frame[i + 2] - wrist_z\n",
    "            normalized_frame.extend([normalized_x, normalized_y, normalized_z])\n",
    "\n",
    "        normalized_landmark_data.append(normalized_frame)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    normalized_landmark_data = np.array(normalized_landmark_data)\n",
    "    \n",
    "    return normalized_landmark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb6a5ac-a4c0-455c-9f45-35e96fb1196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(directory):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,min_detection_confidence=0.4)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    img = cv2.imread(directory)\n",
    "    img = adjust_brightness_contrast(img, 40, 1)\n",
    "    img_rbg =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img_rbg)\n",
    "\n",
    "    sequence = []\n",
    "    sequence_length = 1\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        for lm in result.multi_hand_landmarks[0].landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            img,\n",
    "            result.multi_hand_landmarks[0],\n",
    "            mp_hands.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Append new frame landmarks to sequence\n",
    "\n",
    "        sequence.append(landmarks)\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            sequence_input = np.array(sequence)\n",
    "            sequence_input = normalization_landmarks(sequence_input)\n",
    "            sequence_input = sequence_input.flatten()[np.newaxis, ..., np.newaxis]\n",
    "            prediction = model.predict(sequence_input)\n",
    "            predicted_label_index = np.argmax(prediction)\n",
    "            predicted_label = label_encoder.inverse_transform([predicted_label_index])\n",
    "            confidence = prediction[0][predicted_label_index]\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e352bd-2120-4910-9881-522af182b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data_dir):\n",
    "        mp_hands = mp.solutions.hands\n",
    "        hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.4)\n",
    "\n",
    "\n",
    "        labels1 = []\n",
    "        landmark_data1 = []\n",
    "\n",
    "        for letter in os.listdir(test_data_dir):\n",
    "\n",
    "            letter_dir = os.path.join(test_data_dir, letter)\n",
    "            for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "\n",
    "                img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "                img = adjust_brightness_contrast(img, 40, 1)\n",
    "\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(img_rgb)\n",
    "\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    landmarks = []\n",
    "                    for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                        landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                    landmark_data1.append(landmarks)\n",
    "                    labels1.append(letter)\n",
    "        landmark_data1 = np.array(landmark_data1)\n",
    "        labels1 = np.array(labels1)\n",
    "\n",
    "\n",
    "        # Normalize landmarks between 0 and 1\n",
    "        landmark_data1 = normalization_landmarks(landmark_data1)\n",
    "\n",
    "        # Encode labels as integers and convert to categorical\n",
    "\n",
    "        labels_encoded1 = label_encoder.transform(labels1)\n",
    "        labels_categorical1 = to_categorical(labels_encoded1)\n",
    "        landmark_data1 = np.reshape(landmark_data1,(-1,63,1))\n",
    "\n",
    "        return model.evaluate(landmark_data1,labels_categorical1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0825f1-dfcc-4ad4-8151-445cac482851",
   "metadata": {},
   "source": [
    "## MediaPipe Landmark Creation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a16ee-6c1a-4b00-a145-4ce91643b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Path where images are stored for each letter in the ASL alphabet\n",
    "data_dir = \"../raw_data/asl_alphabet_train/asl_alphabet_train\" # Change according to local dataset\n",
    "\n",
    "landmark_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca97671-58ea-41b5-aedb-346cc28d2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect landmarks for each letter\n",
    "# This function does not normalize inside. We need to run Normalization function outside\n",
    "for letter in os.listdir(data_dir):\n",
    "    #if letter==\"C\":\n",
    "    #    break\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    for i, img_path in enumerate(os.listdir(letter_dir)):\n",
    "        #if i >= 300:\n",
    "        #    break\n",
    "        img = cv2.imread(os.path.join(letter_dir, img_path))\n",
    "        img = adjust_brightness_contrast(img, 40, 1)\n",
    "        #img = adjust_brightness_contrast(img, 20, 0.7)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        \n",
    "        # Check for hand landmarks and store them\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])  # Flattened landmark vector\n",
    "            landmark_data.append(landmarks)\n",
    "            labels.append(letter)  # Store the label (e.g., \"A\", \"B\", etc.)\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc9c0e89-ec69-432e-8c4d-bf09a08ca255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the arrays to .npy files\n",
    "# landmark_data is NOT normalized until now. Only what MediaPipe does.\n",
    "np.save(\"landmark_data_v_large.npy\", landmark_data)\n",
    "np.save(\"labels_v_large.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bababf-26d5-4a0a-920b-0d0095202974",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9123fc-e830-4501-bfb0-4216e8b10629",
   "metadata": {},
   "source": [
    "### If you already have landmark_data and labels you can start running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9249a6ae-5e98-4906-a404-ba1b027e048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load('modeling_stuff/labels_v_large.npy')\n",
    "landmark_data = np.load('modeling_stuff/landmark_data_v_large.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080a4daf-62a7-4e0d-abd8-6338b1438984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding as integers and convert to categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b51a16-509b-4b5b-847b-58a2f051fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Normalization function:\n",
    "normalized_landmarks = normalization_landmarks(landmark_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad916b4-7b0b-4f02-9d7a-7d85c5885da7",
   "metadata": {},
   "source": [
    "### Model Prep\n",
    "\n",
    "This is only if required but as we are using a different test set I will not test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d77cd7-d09c-4a58-b0ba-0bf88833bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_landmarks, labels_categorical, stratify=labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26976d-7b48-4566-b5a6-6c8aad59956f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b53bc5-390d-45c3-9cc0-77a0ceeae3f8",
   "metadata": {},
   "source": [
    "### Building & Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "708ef3c9-3136-4425-978c-537549de2523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # First dense block\n",
    "    Dense(512, input_shape=(63,)),  # Input layer with a high number of neurons\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Second dense block\n",
    "    Dense(512, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Third dense block\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Fourth dense block\n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    # Fifth dense block\n",
    "    Dense(64),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer for classification with softmax activation\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93ac0126-63b4-47f0-ae77-8873625ff756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc6aee-edbe-4392-8699-97c40f2d3517",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe5ca3bf-efdd-4cfd-aaa4-067e6ff0468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_accuracy',\n",
    "                   patience = 5, \n",
    "                   restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10f7147c-db2e-43c0-be6b-b44f38b537a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.7220 - loss: 1.3376 - val_accuracy: 0.9795 - val_loss: 0.2016\n",
      "Epoch 2/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9531 - loss: 0.3134 - val_accuracy: 0.9781 - val_loss: 0.2079\n",
      "Epoch 3/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9609 - loss: 0.2828 - val_accuracy: 0.9878 - val_loss: 0.1743\n",
      "Epoch 4/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9656 - loss: 0.2657 - val_accuracy: 0.9852 - val_loss: 0.1795\n",
      "Epoch 5/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9679 - loss: 0.2606 - val_accuracy: 0.9904 - val_loss: 0.1672\n",
      "Epoch 6/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9712 - loss: 0.2459 - val_accuracy: 0.9894 - val_loss: 0.1635\n",
      "Epoch 7/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9710 - loss: 0.2453 - val_accuracy: 0.9914 - val_loss: 0.1590\n",
      "Epoch 8/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.2442 - val_accuracy: 0.9907 - val_loss: 0.1709\n",
      "Epoch 9/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9733 - loss: 0.2372 - val_accuracy: 0.9883 - val_loss: 0.1673\n",
      "Epoch 10/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9744 - loss: 0.2274 - val_accuracy: 0.9916 - val_loss: 0.1644\n",
      "Epoch 11/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.2199 - val_accuracy: 0.9921 - val_loss: 0.1652\n",
      "Epoch 12/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9755 - loss: 0.2287 - val_accuracy: 0.9925 - val_loss: 0.1526\n",
      "Epoch 13/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.9760 - loss: 0.2239 - val_accuracy: 0.9914 - val_loss: 0.1476\n",
      "Epoch 14/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - accuracy: 0.9769 - loss: 0.2161 - val_accuracy: 0.9920 - val_loss: 0.1423\n",
      "Epoch 15/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.9772 - loss: 0.2068 - val_accuracy: 0.9908 - val_loss: 0.1600\n",
      "Epoch 16/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.2141 - val_accuracy: 0.9922 - val_loss: 0.1478\n",
      "Epoch 17/100\n",
      "\u001b[1m2875/2875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.9775 - loss: 0.2093 - val_accuracy: 0.9925 - val_loss: 0.1533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x33c251c00>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=100, \n",
    "          batch_size=64, \n",
    "          validation_data=(X_test[..., np.newaxis], y_test), \n",
    "          callbacks=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879041c1-1b05-4219-a24f-bfbf8c2c8c3e",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0334f17-4a82-4bf3-8b63-cf03c356f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'asl_new_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff40212-50b5-4bff-8140-a57b0e1bda3a",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f8987-f8e4-4d0b-97e0-adc55f89868d",
   "metadata": {},
   "source": [
    "### If you already have a model. You can load it up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c448db-4b57-45d8-85f4-2ea69c93b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# model = tf.keras.models.load_model(\"old_asl_sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cc1e800-39eb-4ca9-a887-d7d971c8f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731093544.839786 6620178 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1731093544.850211 6674290 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731093544.855598 6674290 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7424 - loss: 1.4007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731093559.559922 6620178 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1731093559.566208 6674442 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731093559.572037 6674442 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/wabe/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7424 - loss: 1.4007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2766408920288086, 0.7753623127937317]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"raw_data/test_set_pics\")\n",
    "evaluate_model(\"raw_data/test_set_pics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2093ed1-4580-45df-8d96-92a3cb6cd519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
